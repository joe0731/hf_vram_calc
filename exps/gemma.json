{
  "model": {
    "name": "google/gemma-3-27b-it",
    "architecture": "gemma3_text",
    "parameters": 28418976512,
    "parameters_formatted": "28.4B",
    "original_torch_dtype": "torch.bfloat16",
    "user_specified_dtype": "FP8"
  },
  "memory_requirements": [
    {
      "dtype": "FP8",
      "batch_size": 1,
      "sequence_length": 2048,
      "lora_rank": 64,
      "model_size_gib": 26.47,
      "kv_cache_size_gib": 0.64,
      "inference_total_gib": 31.76,
      "training_gib": 137.63,
      "lora_size_gib": 33.89
    }
  ]
}